{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarization starting",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrJVns9n6Xs+ECtO3m9Au7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameep114/myproject/blob/master/summarization_starting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPK_RxwliGdP",
        "outputId": "c14f19a4-1fe4-4abe-e688-6d3214228607"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#get gpu deviece name\n",
        "device_name=tf.test.gpu_device_name()\n",
        "\n",
        "#the device name should look likefollowing\n",
        "if device_name=='/device:GPU:0':\n",
        "  print(\"found GPU at:{}\",format(device_name))\n",
        "else:\n",
        "  raise SystemError('Gpu Device Not FOund')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found GPU at:{} /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j3lFkA0ltNy",
        "outputId": "1d8a7940-304d-403c-d254-34642fb3f1e6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# if there is GPU available\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  #requesting Pytorch tof use GPU\n",
        "  device= torch.device(\"cuda\")\n",
        "  print(\"there is %d GPU(s)  available.\" % torch.cuda.device_count())\n",
        "  print(\"we will use the gpu:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "#if not...\n",
        "else:\n",
        "  print('NO GPU Available using cpu instead.')\n",
        "  device=torch.device('Cpu')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there is 1 GPU(s)  available.\n",
            "we will use the gpu: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgOUtcGvnZbr"
      },
      "source": [
        "installing hugging face library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jDAat-0m5QJ",
        "outputId": "36404ec0-d781-4708-fb4f-00b20c3a22af"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 30.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 19.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 16.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 16.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 14.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 14.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 15.5MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 15.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 15.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 15.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 15.5MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 15.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 15.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 15.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=73af7a9d34fdde6cb302ab5d338dd5b395d932cb8073e8cae13a86022c66df8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nFOP8Cqpf-1",
        "outputId": "a63cdccd-40a2-4fd7-d555-89962c1338cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBWlhnivnhQl"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAx28HtKDCb0"
      },
      "source": [
        "TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KauDcYgUDMuJ",
        "outputId": "7f6bab77-2feb-49f3-b600-7800368f9096"
      },
      "source": [
        "print(\"we are now loading our dataset...\")\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Datasets/nepali_news_dataset_20_categories_large/Agriculture/1.txt\",delimiter='\\n' ,header=None,names=['sentences'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we are now loading our dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "pKdSgbEHDdx6",
        "outputId": "0b0609bd-5fdd-4372-e86b-d4fb7e9bbe78"
      },
      "source": [
        "#report number of sentences.\n",
        "print(\"number of training sentences: {:,}\\n\".format(df.shape[0]))\n",
        "\n",
        "# displaying random rows\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sentences: 11\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेत...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>यसअघि स्ट्रबेरी नुवाकोटमा गरिँदै आएको थियो। अध...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>बीस–पच्चीस वर्षअघि ककनीमा जाइटी भन्ने संस्थाले...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>‘मैले नुवाकोटमा ग्रीन हाउस बनाउने थोपा सिँचाइक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>अधिकारीको फर्ममा अहिले स्ट्रबेरी फल्न सुरु गरे...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>५ देखि २५ डिग्री तापक्रममा स्ट्रबेरी लगाउन सकि...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>विभिन्न शुभ कार्यहरुमा समेत स्ट्रबेरीलाई प्याक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>‘नेपालमा पनि स्ट्रबेरीको उत्पादन गर्न सके बजार...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>स्ट्रबेरीको जाम विश्वमै चर्चित रहेको उनले बताए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>विदेशमा लामो समयसम्म काम गरेकोले पनि आफूले स्व...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences\n",
              "0  काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेत...\n",
              "1  यसअघि स्ट्रबेरी नुवाकोटमा गरिँदै आएको थियो। अध...\n",
              "2  बीस–पच्चीस वर्षअघि ककनीमा जाइटी भन्ने संस्थाले...\n",
              "3  ‘मैले नुवाकोटमा ग्रीन हाउस बनाउने थोपा सिँचाइक...\n",
              "4  अधिकारीको फर्ममा अहिले स्ट्रबेरी फल्न सुरु गरे...\n",
              "5  ५ देखि २५ डिग्री तापक्रममा स्ट्रबेरी लगाउन सकि...\n",
              "6  विभिन्न शुभ कार्यहरुमा समेत स्ट्रबेरीलाई प्याक...\n",
              "7  ‘नेपालमा पनि स्ट्रबेरीको उत्पादन गर्न सके बजार...\n",
              "8  स्ट्रबेरीको जाम विश्वमै चर्चित रहेको उनले बताए...\n",
              "9  विदेशमा लामो समयसम्म काम गरेकोले पनि आफूले स्व..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkWV-gWjIlyv"
      },
      "source": [
        "#get list of sentences\n",
        "sentences=df.sentences.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2AEQraxD3nD",
        "outputId": "78e6fd79-2b28-47b6-a85a-8c07345c574b"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "#load bert tokenizer\n",
        "print(\"loading BERT  tokenizer...\")\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading BERT  tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnRJQHQ-GPSQ",
        "outputId": "ac72d8d8-0e95-4ce2-f7d7-f898bdc1ed41"
      },
      "source": [
        "print('original:',sentences[0])\n",
        "\n",
        "# print paragrapg splitted into tokens.\n",
        "print('Tokenized:',tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "#print sentences mapped into token ids\n",
        "print('Token IDs:',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेती सुरु भएको छ । एक दशकसम्म इजरायली कृषि फार्ममा बसेर काम गरेका श्रीकृष्ण अधिकारीले तारकेश्वर नगरपालिका ४ गोलढुंगामा उत्पादन सुरु गरेका हुन्।\n",
            "Tokenized: ['का', '##ठ', '##मा', '##ड', '##ौ', '##ं', '##मा', 'पहिलो', 'प', '##टक', 'स', '##्ट', '##्र', '##बे', '##री', '##को', 'व', '##्य', '##व', '##सा', '##यिक', 'ख', '##ेत', '##ी', 'स', '##ुरु', 'भएको', 'छ', '।', 'एक', 'दशक', '##सम', '##्म', 'इ', '##ज', '##रा', '##य', '##ली', 'क', '##ृ', '##षि', 'फ', '##ार', '##्म', '##मा', 'ब', '##से', '##र', 'काम', 'गरेका', 'श्री', '##क', '##ृ', '##ष', '##्ण', 'अधिकार', '##ील', '##े', 'त', '##ार', '##के', '##श', '##्वर', 'नगर', '##पालिका', '४', 'ग', '##ोल', '##ढ', '##ु', '##ंगा', '##मा', 'उत्पादन', 'स', '##ुरु', 'गरेका', 'हुन्', '।']\n",
            "Token IDs: [11081, 35247, 12347, 20691, 78530, 14018, 12347, 92770, 885, 76826, 898, 25695, 18321, 85002, 20161, 12512, 895, 14251, 15070, 35127, 96823, 866, 53316, 10914, 898, 56902, 31454, 871, 920, 11186, 103202, 105794, 33270, 853, 17413, 31277, 13874, 15658, 865, 111207, 84456, 886, 19885, 33270, 12347, 887, 35622, 11549, 28043, 74747, 38518, 12151, 111207, 39765, 55719, 70103, 28185, 11554, 880, 19885, 38150, 21835, 80731, 52851, 74467, 926, 867, 51140, 111204, 14070, 103792, 12347, 72036, 898, 56902, 74747, 49374, 920]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWGdTBtkJW9i",
        "outputId": "007f87f4-ab9b-46f4-f9a8-155332657c08"
      },
      "source": [
        "# tokenize all of the sentences and map the tokens to their respective word ids\n",
        "input_ids=[]\n",
        "\n",
        "#for every sentences\n",
        "for sent in sentences:\n",
        "  encodded_sent=tokenizer.encode(\n",
        "                    sent, add_special_tokens=True,\n",
        "                        )\n",
        "input_ids.append(encodded_sent)\n",
        "\n",
        "# print first paragraph now as list of ids.\n",
        "print('original:',sentences[0])\n",
        "print('token IDs',input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेती सुरु भएको छ । एक दशकसम्म इजरायली कृषि फार्ममा बसेर काम गरेका श्रीकृष्ण अधिकारीले तारकेश्वर नगरपालिका ४ गोलढुंगामा उत्पादन सुरु गरेका हुन्।\n",
            "token IDs [101, 867, 50346, 11453, 56523, 111194, 13432, 60701, 76131, 881, 65430, 11208, 898, 12878, 28462, 22078, 15801, 32629, 11549, 75315, 11845, 852, 28863, 63401, 11554, 40107, 15399, 26472, 876, 11208, 111194, 28462, 49050, 14070, 12347, 32824, 87291, 866, 53316, 10914, 44069, 47313, 37920, 867, 31277, 43583, 70103, 10914, 12512, 888, 16380, 34231, 871, 920, 29953, 12347, 92770, 18187, 76826, 881, 65430, 11208, 898, 103435, 22078, 84237, 30472, 895, 14251, 15070, 35127, 96823, 72954, 12347, 852, 28863, 18438, 13665, 30693, 66927, 64645, 882, 36471, 10914, 79263, 920, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4nRXZ44NIOY",
        "outputId": "7233c72b-7aaa-4437-e134-f2dccbb66a76"
      },
      "source": [
        "# before padding we need to know which is the longest sentence in our dataset \n",
        "print('max length of sentence',max([len(sen) for sen in input_ids ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length of sentence 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pAjknX_Sqo5",
        "outputId": "462559ca-f196-4a50-fa17-404631ff3e73"
      },
      "source": [
        "# lets use pad_sequencecs utility function in order to do this work\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# lets choose Max_len == 128 and apply the padding\n",
        "MAX_LEN=128\n",
        "\n",
        "print('\\n padding/truncating all sentences to %d values....'% MAX_LEN)\n",
        "\n",
        "print(\"\\n padding token :'{:}', Id :{:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "#pad our input tokens with valie 0.\n",
        "input_ids=pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                         value=0, truncating=\"post\",padding=\"post\")\n",
        "\n",
        "print(\"\\n Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " padding/truncating all sentences to 128 values....\n",
            "\n",
            " padding token :'[PAD]', Id :0\n",
            "\n",
            " Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN4DHe47Uo6y"
      },
      "source": [
        "attention masks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKCh8g4wVJ8h"
      },
      "source": [
        "#create attention masks\n",
        "attention_masks= []\n",
        "#for each sentence\n",
        "for sent in input_ids:\n",
        "  att_mask=[int(token_id > 0) for token_id in sent] #reason: token id> 0 real token, token id== 0 padding\n",
        "  attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNT6L92V2eX"
      },
      "source": [
        "TRAINING AND VALIDATION SPLIT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJvT1s9eWGMs"
      },
      "source": [
        "#training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#use 90% for training and 10 percent for validation\n",
        "train_inputs, validation_inputs, train_labels, validation_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "HqPLC4Wgf3-K",
        "outputId": "d01c0df6-5a7e-4dea-c60d-6e160cd54201"
      },
      "source": [
        "from transformers import  T5Tokenizer, TFT5ForConditionalGeneration\n",
        "model=TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer=tokenizer = T5Tokenizer.from_pretrained('t5-small')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6d753d5c7df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFT5ForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "BpdKxRBAhQg_",
        "outputId": "3dbdafae-4598-45ed-a34a-4bd582c1baa7"
      },
      "source": [
        "text=sentences.strip().replace(\"\\n\",\"\")\n",
        "preprocessed_text=\"summarize: \"+Text\n",
        "tokens_input=tokenizer.encode(Preprocessed_text,return_tensors='tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a98fa15ef036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summarize: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreprocessed_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'strip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Q1lZ-Mh0Fd",
        "outputId": "5f67a2c1-70dd-4e4b-cb3b-a0cecf1cf636"
      },
      "source": [
        "print('original:',sentences[0])\n",
        "\n",
        "# print paragrapg splitted into tokens.\n",
        "print('Tokenized:',tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "#print sentences mapped into token ids\n",
        "print('Token IDs:',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेती सुरु भएको छ । एक दशकसम्म इजरायली कृषि फार्ममा बसेर काम गरेका श्रीकृष्ण अधिकारीले तारकेश्वर नगरपालिका ४ गोलढुंगामा उत्पादन सुरु गरेका हुन्।\n",
            "Tokenized: ['▁', 'काठमाडौंमा', '▁', 'पहिलो', '▁', 'पटक', '▁', 'स्ट्रबेरीको', '▁', 'व्यवसायिक', '▁', 'खेती', '▁', 'सुरु', '▁', 'भएको', '▁', 'छ', '▁', '।', '▁', 'एक', '▁', 'दशकसम्म', '▁', 'इजरायली', '▁', 'कृषि', '▁', 'फार्ममा', '▁', 'बसेर', '▁', 'काम', '▁', 'गरेका', '▁', 'श्रीकृष्ण', '▁', 'अधिकारीले', '▁', 'तारकेश्वर', '▁', 'नगरपालिका', '▁', '४', '▁', 'गोलढुंगामा', '▁', 'उत्पादन', '▁', 'सुरु', '▁', 'गरेका', '▁', 'हुन्।']\n",
            "Token IDs: [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrIl2t1TiPlD",
        "outputId": "b0036a65-2c8b-4fec-c275-63c4eb6d2af8"
      },
      "source": [
        "# tokenize all of the sentences and map the tokens to their respective word ids\n",
        "input_ids=[]\n",
        "\n",
        "#for every sentences\n",
        "for sent in sentences:\n",
        "  encodded_sent=tokenizer.encode(\n",
        "                    sent, add_special_tokens=True,\n",
        "                        )\n",
        "input_ids.append(encodded_sent)\n",
        "\n",
        "# print first paragraph now as list of ids.\n",
        "print('original:',sentences[0])\n",
        "print('token IDs',input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: काठमाडौंमा पहिलो पटक स्ट्रबेरीको व्यवसायिक खेती सुरु भएको छ । एक दशकसम्म इजरायली कृषि फार्ममा बसेर काम गरेका श्रीकृष्ण अधिकारीले तारकेश्वर नगरपालिका ४ गोलढुंगामा उत्पादन सुरु गरेका हुन्।\n",
            "token IDs [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfQdGlH7jZal",
        "outputId": "2d7f4e4f-b854-40d1-b601-d74fe872eca9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_ids=tokenizer.tokenize(sentences[0])\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'काठमाडौंमा', '▁', 'पहिलो', '▁', 'पटक', '▁', 'स्ट्रबेरीको', '▁', 'व्यवसायिक', '▁', 'खेती', '▁', 'सुरु', '▁', 'भएको', '▁', 'छ', '▁', '।', '▁', 'एक', '▁', 'दशकसम्म', '▁', 'इजरायली', '▁', 'कृषि', '▁', 'फार्ममा', '▁', 'बसेर', '▁', 'काम', '▁', 'गरेका', '▁', 'श्रीकृष्ण', '▁', 'अधिकारीले', '▁', 'तारकेश्वर', '▁', 'नगरपालिका', '▁', '४', '▁', 'गोलढुंगामा', '▁', 'उत्पादन', '▁', 'सुरु', '▁', 'गरेका', '▁', 'हुन्।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1EHwPnmn4K7"
      },
      "source": [
        "input_tokens=np.array(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "zuCag8WSicv0",
        "outputId": "d3653f56-e2c8-4d36-c064-b66455f5ff40"
      },
      "source": [
        "summary_ids=model.generate(input_tokens, \n",
        "                           max_length=50)\n",
        "summary=tokenizer.decode(summary_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-2938f71afe2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m summary_ids=model.generate(input_tokens, min_length=30,\n\u001b[0;32m----> 2\u001b[0;31m                            max_length=50)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# overridden by the input batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mshape_list\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;34m\"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mstatic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m     \u001b[0mdynamic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'as_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4OllnsPqP5h",
        "outputId": "0004961f-c780-4f53-c248-e25a9bb8b57e"
      },
      "source": [
        "preprocess_text = sentences.strip().replace(\"\\n\",\"\")\n",
        "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text preprocessed: \n",
            " सहज रुपमा भन्नुपर्दा नेपाली भनेको नेपालमा बस्ने नागरिक हो । हामी नेपाली नेपाललाई अंग्रेजबाट मुक्त गराएर स्वच्छ बनाउने बिर योद्धा हो । नेपाल अनेक जात, अनेक धर्म र अनेक भाषाभेष मिसिएर बनेको छ। हामी नेपालीमा घमण्ड र स्वार्थ भन्ने भावना उठेको छैन। हामी निस्वार्थी र धर्मको बाटोमा लाग्ने मानिस हौ। हामी रिस,राग,कपट र छल कहिले गर्दैनौ। हाम्रो देश स्वर्ग झैँ सुन्दर र चद्रमा झैँ चम्किलो छ। हामी नेपाली एकतामा बसेर हरेक कम गर्छौ र सफलता प्राप्त गर्छौ।                                            हाम्रो देशमा एक पछि एक महान पुरुषहरु जन्मेका हुन्। जस्तै-प्रिथिवी नारायण शाह,बलभद्र कुँवर आदि। उहाँहरुले ठुला ठुला काम गरी जानुभएको छ। उहाँहरुले देशका लागि केही न केही योगदान दिएर गएका हन्। हामी नेपाली तिनीहरुका छोराछोरी हौ र त्यसै कारणले हामीले पनि देशको लागि केही गर्नुपर्छ। हाम्रो देशमा बिर गोर्खालीहरु जन्मेका हुन्, उनिहेरुले शत्रुहरुलाई पराजित गरेर हाम्रो देशलाई जोगाएका हुन्। उनीहरु नेपालमै जन्मेका थिए। उनीहरुको जस्तै हामीले पनि आफ्नो प्राणको चिन्ता नगरी देशलाई हरेक परिस्थितिबाट बचाउनुपर्छ। हामी नेपालीले नेपाललाई विकास तर्फ लानु पर्छ। हामी नेपालीले कर्म मात्र गर्नुपर्छ, फलको उपेक्षा गर्नु हुँदैन। हामी नेपालले विर गोर्खाली रगतले हार मान्दैनौ र गौतम बुद्धको शिक्षालाई चारै तर्फ बाटछौ। हामी नेपालीले आफ्नो ज्ञानले सबैलाई पवित्र बनाउनुपर्छ। हामी नेपालीले दुखसुख बाटी हरेकलाई सहयोग गर्छौ र गरिबलाई खान,कपडा,घर आदि गरिदिन्छौ।हामी नेपाली दयालु छौ र परिश्रम गरी हरेक कम गर्छौ।                                                   हामी नेपालीले एकतामा बसी देशमा परेको हरेक समस्यालाई समाधान गर्छौ। नेपाल आमालै पिर परेको बेला हामी अगाडि बढ्छौ र देशलाई संकटबाट मुक्त गर्छौ। हामीले आफ्नो देशलाई दियोको जस्तो सधैभरिको उज्यालो रक्छौ। त्यसैले हामीले आफ्नो कर्तव्यलाई सधै पुरा गर्नुपर्छ र देशलाई स्वर्ग झैँ सुन्दर बनाउन सक्नुपर्छ । हामी नेपाली अरुको भर पर्दैनौ। हामीले आफुले नै कर्म गरी आफ्नो घरपरिवारलाई पाल्छौ र देशलाई सुन्दर र स्वच्छ राखना प्रयास गर्छौ।यदि तपाइलाई अझै चाहियो भने कृपया comment मा इमेल लेखेर दिनुहोला अथवा subscribe गर्नुहोस ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "enDKxPrPqLdG",
        "outputId": "fb46ae22-de6e-46fd-a201-0ee90b1bebb0"
      },
      "source": [
        "summary_ids = model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100,\n",
        "                                    early_stopping=True)\n",
        "\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8525719202f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     early_stopping=True)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# overridden by the input batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mshape_list\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;34m\"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mstatic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m     \u001b[0mdynamic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'as_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06GRUW5lssYE"
      },
      "source": [
        "text=\"\"\"\n",
        "सहज रुपमा भन्नुपर्दा नेपाली भनेको नेपालमा बस्ने नागरिक हो । हामी नेपाली नेपाललाई अंग्रेजबाट मुक्त गराएर स्वच्छ बनाउने बिर योद्धा हो । नेपाल अनेक जात, अनेक धर्म र अनेक भाषाभेष मिसिएर बनेको छ। हामी नेपालीमा घमण्ड र स्वार्थ भन्ने भावना उठेको छैन। हामी निस्वार्थी र धर्मको बाटोमा लाग्ने मानिस हौ। हामी रिस,राग,कपट र छल कहिले गर्दैनौ। हाम्रो देश स्वर्ग झैँ सुन्दर र चद्रमा झैँ चम्किलो छ। हामी नेपाली एकतामा बसेर हरेक कम गर्छौ र सफलता प्राप्त गर्छौ।\n",
        "                                            हाम्रो देशमा एक पछि एक महान पुरुषहरु जन्मेका हुन्। जस्तै-प्रिथिवी नारायण शाह,बलभद्र कुँवर आदि। उहाँहरुले ठुला ठुला काम गरी जानुभएको छ। उहाँहरुले देशका लागि केही न केही योगदान दिएर गएका हन्। हामी नेपाली तिनीहरुका छोराछोरी हौ र त्यसै कारणले हामीले पनि देशको लागि केही गर्नुपर्छ। हाम्रो देशमा बिर गोर्खालीहरु जन्मेका हुन्, उनिहेरुले शत्रुहरुलाई पराजित गरेर हाम्रो देशलाई जोगाएका हुन्। उनीहरु नेपालमै जन्मेका थिए। उनीहरुको जस्तै हामीले पनि आफ्नो प्राणको चिन्ता नगरी देशलाई हरेक परिस्थितिबाट बचाउनुपर्छ। हामी नेपालीले नेपाललाई विकास तर्फ लानु पर्छ। हामी नेपालीले कर्म मात्र गर्नुपर्छ, फलको उपेक्षा गर्नु हुँदैन। हामी नेपालले विर गोर्खाली रगतले हार मान्दैनौ र गौतम बुद्धको शिक्षालाई चारै तर्फ बाटछौ। हामी नेपालीले आफ्नो ज्ञानले सबैलाई पवित्र बनाउनुपर्छ। हामी नेपालीले दुखसुख बाटी हरेकलाई सहयोग गर्छौ र गरिबलाई खान,कपडा,घर आदि गरिदिन्छौ।हामी नेपाली दयालु छौ र परिश्रम गरी हरेक कम गर्छौ।\n",
        "                                                  \n",
        " हामी नेपालीले एकतामा बसी देशमा परेको हरेक समस्यालाई समाधान गर्छौ। नेपाल आमालै पिर परेको बेला हामी अगाडि बढ्छौ र देशलाई संकटबाट मुक्त गर्छौ। हामीले आफ्नो देशलाई दियोको जस्तो सधैभरिको उज्यालो रक्छौ। त्यसैले हामीले आफ्नो कर्तव्यलाई सधै पुरा गर्नुपर्छ र देशलाई स्वर्ग झैँ सुन्दर बनाउन सक्नुपर्छ । हामी नेपाली अरुको भर पर्दैनौ। हामीले आफुले नै कर्म गरी आफ्नो घरपरिवारलाई पाल्छौ र देशलाई सुन्दर र स्वच्छ राखना प्रयास गर्छौ।\n",
        "\n",
        "यदि तपाइलाई अझै चाहियो भने कृपया comment मा इमेल लेखेर दिनुहोला अथवा subscribe गर्नुहोस ।\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34o6ULU7s3lD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "eee0f39c-d08c-4803-9494-1ac7c78d7ae9"
      },
      "source": [
        "Text = text.strip().replace(\"\\n\",\"\")\n",
        "Preprocessed_text = \"summarize: \"+Text\n",
        "tokens_input = tokenizer.encode(Preprocessed_text,return_tensors=\"tf\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ae2322096dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPreprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"summarize: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreprocessed_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Pbbve2tYWi"
      },
      "source": [
        "summary_ids = model.generate(tokens_input,\n",
        "                             min_length=60,\n",
        "                             max_length=80,\n",
        "                             )\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hym3YW_HtdBQ",
        "outputId": "6c7c00c1-abb9-4b0f-f954-7167eb11f70e"
      },
      "source": [
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z500KHjeticc"
      },
      "source": [
        "text=\"\"\"During school days, I have developed an interest in playing games, staging dramas, debating and other extra-curricular activities.\n",
        "\n",
        "I am a good debater and orator, athlete, and sportsman and at the same time, I hold top positions in the class in the academic field. All the qualities of head and heart have earned me a profound love and respect from my teachers and friends. My teachers encourage and help me in all possible ways. I am in the good books of all the teachers as well as the principal because I have won many medals, trophies, shields, and certificates for my extraordinary display of abilities in examinations, athletics, debates, and theatre.\n",
        "\n",
        "The good ideas-like love for the motherland, devotion to duty, obedience- towards elders, service to the nation, helping the poor and the needy, nursing the sick, feeding the hungry, etc – are inculcated in the students during their school days.\n",
        "\n",
        "Broadly speaking, school life is not only for learning, reading books or playing, but also a period during which all the good habits are acquired, bad habits are shunned and good conduct, fair play, and sound thinking are developed. Further, the healthy ideas of patriotism and nationalism are imbibed by the students during this period.\n",
        "\n",
        "My school life shall prepare me for a sound and firm foundation upon which the building of my life is going to be erected. My mistakes and failures would guide me in my future life. I do not feel discouraged and disheartened because for me the failures are the stepping stones to success. My school life is a great and valuable experience, the best teacher. I shall always remember my school days fondly.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fio36uxK6xby"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}